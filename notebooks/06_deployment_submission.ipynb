{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "999d05f0",
      "metadata": {},
      "source": [
        "# Deployment & Submission: Titanic Machine Learning from Disaster\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook documents the **Deployment** phase (CRISP-DM Phase 6) for the Titanic Kaggle competition. It covers the process of generating, validating, and submitting predictions, archiving artifacts, and logging results for reproducibility and business reporting.\n",
        "\n",
        "---\n",
        "**CRISP-DM Phase 6 of 6** | **Previous:** [Evaluation](05_evaluation.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e99d60",
      "metadata": {},
      "source": [
        "## 1. Submission Workflow & Checklist\n",
        "\n",
        "**Deployment Steps:**\n",
        "1. Generate predictions for the test set using the saved preprocessor and final model.\n",
        "2. Format the submission file:\n",
        "   - Exactly 2 columns: `PassengerId`, `Survived`\n",
        "   - 418 rows (matches `test.csv`)\n",
        "   - No extra columns or index; `Survived` as integer {0,1}\n",
        "3. Save submission as `submission/submission_YYYYMMDD_modelname.csv`.\n",
        "4. Validate file format and completeness.\n",
        "5. Submit to Kaggle and log leaderboard score.\n",
        "6. Archive model, preprocessor, and notebook for reproducibility.\n",
        "\n",
        "**Reference:** See planning.md for full checklist and code snippets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e76c39a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions for test set using saved preprocessor and final model\n",
        "import pandas as pd\n",
        "from joblib import load\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "test_path = Path('../data/raw/test.csv')\n",
        "model_path = Path('../models/final_model.pkl')\n",
        "preprocessor_path = Path('../data/processed/preprocessor.pkl')\n",
        "\n",
        "# Check for required files\n",
        "if not test_path.exists():\n",
        "    print(f'ERROR: Test data not found at {test_path}. Please add test.csv to this location.')\n",
        "elif not model_path.exists():\n",
        "    print(f'ERROR: Model not found at {model_path}. Please train and save your model.')\n",
        "elif not preprocessor_path.exists():\n",
        "    print(f'ERROR: Preprocessor not found at {preprocessor_path}. Please ensure data preparation step was completed.')\n",
        "else:\n",
        "    # Load artifacts\n",
        "    test = pd.read_csv(test_path)\n",
        "    preprocessor = load(preprocessor_path)\n",
        "    model = load(model_path)\n",
        "\n",
        "    # Transform and predict\n",
        "    X_test = preprocessor.transform(test)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # Format submission file\n",
        "    submission = pd.DataFrame({\n",
        "        'PassengerId': test['PassengerId'],\n",
        "        'Survived': preds.astype(int)\n",
        "    })\n",
        "\n",
        "    # Save with timestamp and model name\n",
        "    today = datetime.today().strftime('%Y%m%d')\n",
        "    model_name = 'gbdt'  # Update if needed\n",
        "    submission_path = Path('submission') / f'submission_{today}_{model_name}.csv'\n",
        "    submission.to_csv(submission_path, index=False)\n",
        "\n",
        "    print(f'Submission file saved to {submission_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e541f451",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate submission file format\n",
        "try:\n",
        "    submission\n",
        "except NameError:\n",
        "    print('ERROR: Submission file not created. Please run the previous cell and ensure required files are present.')\n",
        "else:\n",
        "    assert submission.shape == (418, 2), 'Submission must have 418 rows and 2 columns.'\n",
        "    assert set(submission.columns) == {'PassengerId', 'Survived'}, 'Columns must be PassengerId and Survived.'\n",
        "    assert submission['Survived'].isin([0,1]).all(), 'Survived must be 0 or 1.'\n",
        "    print('Submission file format validated.')\n",
        "\n",
        "    # Log leaderboard score (manual step after Kaggle submission)\n",
        "    lb_score = None  # Fill in after submission\n",
        "    print(f'Kaggle LB score: {lb_score if lb_score else \"<to be filled after submission>\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b338eb18",
      "metadata": {},
      "source": [
        "## 2. Archiving & Reproducibility\n",
        "\n",
        "- Archive the final model, preprocessor, feature columns, and submission file.\n",
        "- Save the deployment notebook and code artifacts for future reference.\n",
        "- Document any changes to features, model parameters, or validation strategy.\n",
        "- Maintain a log of leaderboard scores and notes on each submission.\n",
        "- Ensure all steps are reproducible from raw data to submission.\n",
        "\n",
        "**Professional Takeaway:**\n",
        "This deployment workflow ensures robust, transparent, and reproducible submission for the Titanic Kaggle competition, aligning with CRISP-DM and business requirements.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}